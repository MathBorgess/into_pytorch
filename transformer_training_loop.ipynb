{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy\n",
    "%python -m spacy download en\n",
    "%python -m spacy download de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import spacy\n",
    "import torchtext\n",
    "from torchtext.datasets import Multi30k\n",
    "from collections import Counter\n",
    "#https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_base = 'https://raw.githubusercontent.com/multi30k/dataset/master/data/task1/raw/'\n",
    "train_urls = ('train.de.gz', 'train.en.gz')\n",
    "val_urls = ('val.de.gz', 'val.en.gz')\n",
    "test_urls = ('test_2016_flickr.de.gz', 'test_2016_flickr.en.gz')\n",
    "\n",
    "train_filepaths = [torchtext.utils.extract_archive(\n",
    "    torchtext.utils.download_from_url(\n",
    "        url_base+train_url\n",
    "    )\n",
    ")[0] for train_url in train_urls]\n",
    "\n",
    "val_filepaths = [torchtext.utils.extract_archive(\n",
    "    torchtext.utils.download_from_url(\n",
    "        url_base+val_url\n",
    "    )\n",
    ")[0] for val_url in val_urls]\n",
    "\n",
    "test_filepaths = [torchtext.utils.extract_archive(\n",
    "    torchtext.utils.download_from_url(\n",
    "        url_base+test_url\n",
    "    )\n",
    ")[0] for test_url in test_urls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = torchtext.data.get_tokenizer('spacy', language='en')\n",
    "de_tokenizer = torchtext.data.get_tokenizer('spacy', language='de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "def build_vocab(filepath, tokenizer):\n",
    "    counter = Counter()\n",
    "    with io.open(filepath, encoding=\"utf-8\") as file:\n",
    "        for string_ in file:\n",
    "            counter.update(tokenizer(string_))\n",
    "    return torchtext.vocab.Vocab(counter, specials=['<unk>', '<pad>', '<bos>', '<eos>'])\n",
    "\n",
    "de_vocab, en_vocab = build_vocab(train_filepaths[0], de_tokenizer), build_vocab(train_filepaths[1], en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_process(filepaths):\n",
    "  raw_de_iter = iter(io.open(filepaths[0], encoding=\"utf8\"))\n",
    "  raw_en_iter = iter(io.open(filepaths[1], encoding=\"utf8\"))\n",
    "  data = []\n",
    "  for (raw_de, raw_en) in zip(raw_de_iter, raw_en_iter):\n",
    "    de_tensor_ = torch.tensor([de_vocab[token] for token in de_tokenizer(raw_de)],\n",
    "                            dtype=torch.long)\n",
    "    en_tensor_ = torch.tensor([en_vocab[token] for token in en_tokenizer(raw_en)],\n",
    "                            dtype=torch.long)\n",
    "    data.append((de_tensor_, en_tensor_))\n",
    "  return data\n",
    "\n",
    "train_data = data_process(train_filepaths)\n",
    "val_data = data_process(val_filepaths)\n",
    "test_data = data_process(test_filepaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "PAD_IDX = de_vocab['<pad>']\n",
    "BOS_IDX = de_vocab['<bos>']\n",
    "EOS_IDX = de_vocab['<eos>']\n",
    "\n",
    "def generate_batch(data_batch):\n",
    "  de_batch, en_batch = [], []\n",
    "  for (de_item, en_item) in data_batch:\n",
    "    de_batch.append(torch.cat([torch.tensor([BOS_IDX]), de_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "    en_batch.append(torch.cat([torch.tensor([BOS_IDX]), en_item, torch.tensor([EOS_IDX])], dim=0))\n",
    "  de_batch = torch.nn.utils.rnn.pad_sequence(de_batch, padding_value=PAD_IDX)\n",
    "  en_batch = torch.nn.utils.rnn.pad_sequence(en_batch, padding_value=PAD_IDX)\n",
    "  return de_batch, en_batch\n",
    "\n",
    "train_iter = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "valid_iter = torch.utils.data.DataLoader(val_data, batch_size=BATCH_SIZE,\n",
    "                        shuffle=True, collate_fn=generate_batch)\n",
    "test_iter = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE,\n",
    "                       shuffle=True, collate_fn=generate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Translator(nn.Module):\n",
    "    def __init__(self, \n",
    "                 layers_units, \n",
    "                 dim_model,\n",
    "                 heads,\n",
    "                 src_vocab_size, \n",
    "                 tar_vocab_size, \n",
    "                 src_pad_idx,\n",
    "                 forward_expansion, \n",
    "                 device, \n",
    "                 max_length,\n",
    "                 dropout):\n",
    "        super(Translator, self).__init__()\n",
    "        self.src_embedding = nn.Embedding(src_vocab_size,dim_model)\n",
    "        self.src_pos_embedding = nn.Embedding(max_length, dim_model)\n",
    "\n",
    "        self.tar_embedding = nn.Embedding(tar_vocab_size,dim_model)\n",
    "        self.tar_pos_embedding = nn.Embedding(max_length, dim_model)\n",
    "\n",
    "        self.device = device\n",
    "\n",
    "        self.transformer = nn.Transformer(dim_model,\n",
    "                                        heads,\n",
    "                                        layers_units,\n",
    "                                        layers_units,\n",
    "                                        forward_expansion,\n",
    "                                        dropout)\n",
    "        self.fc_out = nn.Linear(dim_model, tar_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.src_pad_idx = src_pad_idx\n",
    "    \n",
    "    def create_src_mask(self, src):\n",
    "        # the torch src_mask for transformer needs to be transposed\n",
    "        src_mask = src.transpose(0, 1) == self.src_pad_idx\n",
    "        return src_mask\n",
    "    \n",
    "    def forward(self, src, tar):\n",
    "        src_seq_length, N = src.shape\n",
    "        tar_seq_length, N = tar.shape\n",
    "\n",
    "\n",
    "        src_pos = (\n",
    "            torch.arange(0, src_seq_length).unsqueeze(1)\n",
    "            .expand(tar_seq_length, N)\n",
    "            .to(self.device)\n",
    "        )\n",
    "        tar_pos = (\n",
    "            torch.arange(0, tar_seq_length).unsqueeze(1)\n",
    "            .expand(tar_seq_length, N)\n",
    "            .to(self.device)\n",
    "        )\n",
    "\n",
    "        embed_src = self.dropout(\n",
    "            (self.src_embedding(src) + self.src_pos_embedding(src_pos))\n",
    "        )\n",
    "        embed_tar = self.dropout(\n",
    "            (self.tar_embedding(src) + self.tar_pos_embedding(tar_pos))\n",
    "        )\n",
    "\n",
    "        src_padding_mask = self.create_src_mask(src)\n",
    "        tar_mask = self.transformer.generate_square_subsequent_mask(tar_seq_length).to(self.device)\n",
    "\n",
    "        return self.transformer(\n",
    "            embed_src,\n",
    "            embed_tar,\n",
    "            src_key_padding_mask= src_padding_mask,\n",
    "            tgt_mask=tar_mask\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "epochs = 5\n",
    "lr =  1e-4\n",
    "dim_model = 512\n",
    "heads = 8\n",
    "layers_units = 3\n",
    "dropout = 0.1\n",
    "max_length = 100\n",
    "forward_expansion = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter()\n",
    "step = 0\n",
    "\n",
    "model = Translator(\n",
    "    layers_units,\n",
    "    dim_model,\n",
    "    heads,\n",
    "    len(de_vocab),\n",
    "    len(en_vocab),\n",
    "    PAD_IDX,\n",
    "    forward_expansion,\n",
    "    device,\n",
    "    max_length,\n",
    "    dropout\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr)\n",
    "criteion = nn.CrossEntropyLoss(ignore_index=en_vocab['<pad>'])\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch: {epoch} / {epochs}')\n",
    "\n",
    "    model.train()\n",
    "    \n",
    "    for _, (src, tar) in enumerate(train_iter):\n",
    "        src, tar = src.to(device), tar.to(device)\n",
    "        output = model(src, tar[:-1])\n",
    "\n",
    "        output = output.reshape(-1, output.shape[2])\n",
    "        tar = tar[1:].reshape(-1)\n",
    "        optimizer.zero_grad()\n",
    "        loss = criteion(output, tar)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad.clip_grad_norm(model.parameters(), max_norm= 1)\n",
    "        optimizer.step()\n",
    "\n",
    "        writer.add_scalar(\"training loss\", loss, global_step=step)\n",
    "        step += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
